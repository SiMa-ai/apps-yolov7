{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95bf20ec",
   "metadata": {},
   "source": [
    "# AWS SageMaker JupyterLab & SiMa.ai's Palette Software"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519e44a7",
   "metadata": {},
   "source": [
    "To kickstart our workflow, we need to ensure that our environment is **configured correctly**. Specifically, we'll be verifying the integration of the necessary software libraries with NVIDIA’s CUDA platform, as this will be essential for re-training our model efficiently.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1963f3d2-1068-4953-bf0e-ae720c2b1a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34552f0e",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "With the packages verified and configured, let's proceed by cloning the **YOLOv7** repository, which we’ll use to perform machine learning inference.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42521ea-4f6c-4506-8885-f4603f068b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/WongKinYiu/yolov7.git"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512b5816-5c24-45a6-b39d-9fea11a05f7d",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "**Use Case: PPE Detection**  \n",
    "We’re addressing a **Personal Protective Equipment (PPE)** scenario, focusing on detecting people wearing helmets. This means our chosen classes (`Person`, `Human head`, `Helmet`, and `Glasses`) allow us to track whether a person has a helmet on while in a work environment, helping ensure safety compliance.\n",
    "\n",
    "**Data Splits (75% Train, 25% Val, 0% Test)**  \n",
    "We’re dedicating \\(75\\%\\) of our dataset to training and \\(25\\%\\) to validation. We have intentionally set the test split to \\(0\\%\\) because our final pipeline validation is performed on a separate video (instead of a typical test dataset). This approach keeps our training process focused on building a robust model while leaving the ultimate performance verification to real-world video input.\n",
    "\n",
    "**Choosing `model_flavor`: `yolov7` vs. `yolov7-tiny`**  \n",
    "By default, we set `model_flavor` to `\"yolov7-tiny\"`. However, you can switch to `\"yolov7\"` for an increase decrease in accuracy and robustness compared to the smaller YOLOv7-tiny model but YOLOv7 takes **substantially longer to train**, which is important to consider if you’re working under time constraints. `\"yolov7-tiny\"` is smaller, faster model that generally requires less training time and fewer computing resources.\n",
    "\n",
    "***\n",
    "One of the powerful features here is the flexibility to change the `classes` variable to any of the **600** available classes. This allows your model to be re-trained and deployed to the edge, automatically tuned to detect only the specified classes, running seamlessly on a real device.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de3027-eadf-4de3-90ad-5d5620c6d5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set dataset settings\n",
    "\n",
    "classes = [\"Person\", \"Human head\", \"Helmet\", \"Glasses\"]\n",
    "split_percentages = [0.75, 0.25, 0.00]  # Train, Val, Test splits\n",
    "model_flavor = \"yolov7-tiny\"\n",
    "s3_bucket = \"<PASTE YOUR S3 NAME>\"\n",
    "print(f\"S3 Bucket: {s3_bucket}\")\n",
    "if s3_bucket == \"<PASTE YOUR S3 NAME>\":\n",
    "    raise Exception(\"Please update `s3_bucket` with your s3 bucket name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78438149",
   "metadata": {},
   "source": [
    "***\n",
    "## Dataset Creation\n",
    "To begin fine-tuning our model, we'll need a dataset. Here, we'll use the open-source dataset **Open Images V7**. With FiftyOne's seamless integration, we can easily select any class from this dataset to tailor our training process.\n",
    "\n",
    "In this Cell we display all the 600 classes users can choose from:\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b62b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import fiftyone as fo\n",
    "import fiftyone.zoo as foz\n",
    "from fiftyone import ViewField as F\n",
    "import fiftyone.utils.openimages\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "fo.utils.openimages.get_classes(\"v7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4f0fca",
   "metadata": {},
   "source": [
    "***\n",
    "This function, `divide_train`, prepares our dataset for model fine-tuning by loading a specified number of samples from the Open Images v7 dataset, filtering by the chosen `classes`. It exports the dataset in `YOLOv5` format and then organizes the images and labels into structured folders (`train`, `test`, and `val`). This setup ensures that each data split is correctly formatted, with the dataset ready for training, testing, and validation steps, aligning images with corresponding labels across each split. Finally, the dataset file paths are recorded for streamlined access during model training and deployment.\n",
    "\n",
    "Before we load the subsets for each class, there are two important points to highlight regarding *dataset balancing*:\n",
    "\n",
    "1. We **exclude the `Person` class** from our `target_classes`. Since “Helmet” and “Human head” labels naturally involve people, we don’t need to explicitly include “Person” again. Doing so would lead to “Person” being over-represented in the dataset, skewing the class distribution and potentially biasing our model. By removing “Person,” we ensure that images already containing helmets or heads (which imply people) are sufficient to capture person-related data.\n",
    "\n",
    "2. We **limit the number of samples per class** (`samples_per_class`). By setting `max_samples=samples_per_class` for each target class, we make sure each class is represented evenly in the final dataset. This step prevents one class from dominating the training set and helps maintain a balanced distribution—an essential aspect for models that must accurately detect multiple object categories.\n",
    "\n",
    "Below is the code snippet demonstrating these two actions:\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d0874",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "BATCH_SIZE = 16  # Define your batch size here\n",
    "target_classes = classes.copy()\n",
    "target_classes.remove(\"Person\")\n",
    "samples_per_class = 8000  # Load at least 8,000 per class\n",
    "\n",
    "# Ensure dataset size is divisible by BATCH_SIZE\n",
    "# total_images = (total_images // BATCH_SIZE) * BATCH_SIZE\n",
    "\n",
    "# Define paths\n",
    "root = Path(\"new_data\")\n",
    "r_lab = root / \"labels\"\n",
    "r_img = root / \"images\"\n",
    "\n",
    "dataset_name = \"open-images-v7\"\n",
    "open_images_split = \"train\"\n",
    "\n",
    "tr, va, te = 'train', 'val', 'test'\n",
    "fold_names = [tr, va, te]\n",
    "\n",
    "# Create directories for labels and images\n",
    "for path in [r_lab, r_img]:\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "    for split in fold_names:\n",
    "        (path / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Load subsets for each class\n",
    "datasets = []\n",
    "for cls in target_classes:\n",
    "    print(f\"Loading {samples_per_class} images for class: {cls}\")\n",
    "    dataset = foz.load_zoo_dataset(\n",
    "        dataset_name,\n",
    "        split=open_images_split,\n",
    "        label_types=[\"detections\"],\n",
    "        classes=[cls],\n",
    "        max_samples=samples_per_class,\n",
    "        dataset_name=f\"open-images-v7-{cls}-{samples_per_class}\",\n",
    "    )\n",
    "    datasets.append(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fccf5b4-5095-4888-90d1-b1cb0574b0f5",
   "metadata": {},
   "source": [
    "In this stage, we **merge** all the previously loaded subsets (`datasets`) into a single `full_dataset`. We also enforce consistency in two ways: First, we remove existing duplicates by deleting a prior dataset named `\"full_dataset\"`—if any—and re-creating it. Then we take care to ensure the dataset’s total size remains divisible by our specified `BATCH_SIZE`. This maintains cleaner batch boundaries in downstream training steps.  \n",
    "\n",
    "Next, we perform a series of **label manipulations and dataset exports**. We relabel “Man” and “Woman” to “Person,” and convert “Goggles”/“Sunglasses” into “Glasses.” We also add a mechanism to identify if a `Helmet`-bounding box is fully inside a `Person` bounding box that **does not** already contain a `Human head`—in which case we treat that Helmet detection as a new `Human head` (capturing a head + helmet situation). After finalizing these bounding-box labels, we remove duplicates and split our dataset into train, val, and test sets according to `split_percentages`. Finally, we export each set to the YOLOv5 format, copy its images, and write file paths into `.txt` split files. This results in a well-structured, deduplicated dataset—ready for model training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e17940b-facd-4dd2-93df-2351f9de6884",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Combine all subsets into one dataset\n",
    "full_dataset = fo.Dataset(\"full_dataset\")\n",
    "for dataset in datasets:\n",
    "    full_dataset.merge_samples(dataset)\n",
    "\n",
    "# Ensure dataset size is divisible by BATCH_SIZE\n",
    "final_size = (full_dataset.count() // BATCH_SIZE) * BATCH_SIZE\n",
    "full_dataset = full_dataset.take(final_size)\n",
    "\n",
    "\n",
    "# Helper function to check if one bounding box is within another\n",
    "def is_within_box(inner_box, outer_box):\n",
    "    \"\"\"\n",
    "    Check if a bounding box (inner_box) is fully within another bounding box (outer_box).\n",
    "\n",
    "    Args:\n",
    "        inner_box (list): [x, y, width, height] of the inner box.\n",
    "        outer_box (list): [x, y, width, height] of the outer box.\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the inner box is within the outer box, False otherwise.\n",
    "    \"\"\"\n",
    "    ix, iy, iw, ih = inner_box\n",
    "    ox, oy, ow, oh = outer_box\n",
    "\n",
    "    return ix >= ox and iy >= oy and (ix + iw) <= (ox + ow) and (iy + ih) <= (oy + oh)\n",
    "\n",
    "# Relabel Goggles to Glasses and process Human head and Person labels\n",
    "for sample in full_dataset:\n",
    "    person_boxes = []\n",
    "    human_head_boxes = []\n",
    "    new_detections = []\n",
    "\n",
    "    for detection in sample.ground_truth.detections:\n",
    "        # Relabel \"Man\" and \"Woman\" to \"Person\"\n",
    "        if detection.label in [\"Man\", \"Woman\"]:\n",
    "            detection.label = \"Person\"\n",
    "\n",
    "        # Relabel \"Goggles\" to \"Glasses\"\n",
    "        if detection.label in [\"Goggles\", \"Sunglasses\"]:\n",
    "            detection.label = \"Glasses\"\n",
    "\n",
    "        # Collect \"Person\" and \"Human head\" bounding boxes\n",
    "        if detection.label == \"Person\":\n",
    "            person_boxes.append(detection.bounding_box)\n",
    "        elif detection.label == \"Human head\":\n",
    "            human_head_boxes.append(detection.bounding_box)\n",
    "\n",
    "    # Process \"Helmet\" labels and check conditions\n",
    "    for detection in sample.ground_truth.detections:\n",
    "        if detection.label == \"Helmet\":\n",
    "            helmet_box = detection.bounding_box\n",
    "            for person_box in person_boxes:\n",
    "                if is_within_box(helmet_box, person_box) and not any(\n",
    "                    is_within_box(human_head_box, person_box) for human_head_box in human_head_boxes\n",
    "                ):\n",
    "                    # Create a new detection for Human head\n",
    "                    new_detection = detection.copy()  # Copy the Helmet detection\n",
    "                    new_detection.label = \"Human head\"  # Change label\n",
    "                    new_detections.append(new_detection)\n",
    "\n",
    "    # Add the new detections to the sample\n",
    "    sample.ground_truth.detections.extend(new_detections)\n",
    "    sample.save()  # Save changes for each sample\n",
    "\n",
    "\n",
    "n_train_split = int(split_percentages[0] * final_size)\n",
    "n_val_split = int(split_percentages[1] * final_size)\n",
    "n_test_split = final_size - n_train_split - n_val_split\n",
    "\n",
    "# Split dataset\n",
    "splits = {\n",
    "    tr: full_dataset.take(n_train_split),\n",
    "    va: full_dataset.skip(n_train_split).take(n_val_split),\n",
    "    te: full_dataset.skip(n_train_split + n_val_split).take(n_test_split),\n",
    "}\n",
    "\n",
    "# Define a function to copy images to their respective split folders\n",
    "def copy_images_to_split(split_name, split_view):\n",
    "    split_img_dir = r_img / split_name\n",
    "    # Ensure the target directory exists\n",
    "    split_img_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    for sample in split_view:\n",
    "        img_src_path = Path(sample.filepath)\n",
    "        img_dst_path = split_img_dir / img_src_path.name\n",
    "        # Check if the source file exists\n",
    "        if not img_src_path.exists():\n",
    "            print(f\"Source file does not exist: {img_src_path}\")\n",
    "            continue  # Skip missing files\n",
    "        shutil.copy(img_src_path, img_dst_path)\n",
    "\n",
    "# Remove duplicate labels from the dataset\n",
    "def remove_duplicate_labels(dataset, label_field):\n",
    "    for sample in dataset:\n",
    "        if label_field in sample and sample[label_field] is not None:\n",
    "            detections = sample[label_field]\n",
    "            # Use a dictionary to keep unique detections by label and bounding box\n",
    "            unique_detections = []\n",
    "            seen = set()\n",
    "            for det in detections.detections:  # Access the detections list\n",
    "                key = (det.label, tuple(det.bounding_box))\n",
    "                if key not in seen:\n",
    "                    seen.add(key)\n",
    "                    unique_detections.append(det)\n",
    "            # Update the sample with all unique detections\n",
    "            detections.detections = unique_detections\n",
    "            sample[label_field] = detections\n",
    "            sample.save()\n",
    "\n",
    "# # Apply the function to the balanced dataset\n",
    "remove_duplicate_labels(full_dataset, \"ground_truth\")\n",
    "\n",
    "# Export and create split files\n",
    "for split_name, split_view in splits.items():\n",
    "    split_img_dir = r_img / split_name\n",
    "    labels_dir = r_lab / split_name\n",
    "    split_txt = root / f\"{split_name}.txt\"\n",
    "\n",
    "    # Export labels to YOLOv5 format\n",
    "    split_view.export(\n",
    "        dataset_type=fo.types.YOLOv5Dataset,\n",
    "        labels_path=str(labels_dir),\n",
    "        classes=classes,  # Correctly ensure target classes are used\n",
    "    )\n",
    "\n",
    "    # Copy images to split directory\n",
    "    print(f\"Copying images for split: {split_name}\")\n",
    "    copy_images_to_split(split_name, split_view)\n",
    "\n",
    "    # Write image paths to split file\n",
    "    with open(split_txt, \"w\") as f:\n",
    "        for sample in split_view:\n",
    "            img_relative_path = f\"./images/{split_name}/{Path(sample.filepath).name}\"\n",
    "            f.write(f\"{img_relative_path}\\n\")\n",
    "\n",
    "    # Debugging: Check split sizes\n",
    "    print(f\"Split '{split_name}' contains {split_view.count()} images.\")\n",
    "\n",
    "print(\"Dataset split and export completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e745e619",
   "metadata": {},
   "source": [
    "***\n",
    "# YoloV7 Model Training\n",
    "\n",
    "This section configures the YOLOv7 model and downloads its pre-trained weights if necessary:\n",
    "\n",
    "1. **Configuration File Setup**  \n",
    "   - We define `config_path` to point to a specific YOLOv7 `.yaml` file, either `yolov7.yaml` or `yolov7-tiny.yaml` depending on the selected `model_flavor`. We also build a URL (`file_url`) for the corresponding pre-trained weights (`model_flavor.pt`) and set a `file_path` location to store them locally.  \n",
    "   - Next, we list our detection classes (`[\"Person\", \"Human head\", \"Helmet\", \"Glasses\", \"Woman\"]`) and then create a `custom.yaml` that YOLOv7 will use. This file references our train/val/test `.txt` splits and specifies the number of classes (`nc`) plus the class names (`names`). Storing this in `yolov7/data/custom.yaml` allows YOLOv7 to find our dataset details when we begin training.\n",
    "\n",
    "2. **Updating the Number of Classes and Managing Model Weights**  \n",
    "   - We load the original `model_flavor.yaml` file (in `yolov7/cfg/training/`) and modify the line that sets `nc:` (number of classes), ensuring it matches `len(classes)` from our custom dataset. This step guarantees the model architecture is aligned with our label set.  \n",
    "   - Finally, we check whether the corresponding `.pt` weight file already exists. If it doesn’t, we create the `yolov7` directory (if not present) and download the specified weights via `wget`. Otherwise, we skip the download and log that the file is already available. This process prepares our environment with both the dataset configuration (`custom.yaml`) and the correct pre-trained weights (`.pt`), ready for fine-tuning or training.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0336cbac-2249-4670-a056-a1512f948145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the config file\n",
    "config_path = f\"yolov7/cfg/training/{model_flavor}.yaml\"  # Update this with the path to your config file\n",
    "file_url = f\"https://github.com/WongKinYiu/yolov7/releases/download/v0.1/{model_flavor}.pt\"\n",
    "file_path = f\"yolov7/{model_flavor}.pt\"\n",
    "\n",
    "classes = [\"Person\", \"Human head\", \"Helmet\", \"Glasses\", \"Woman\"]\n",
    "\n",
    "custom_yaml = f\"{tr}: ../{root}/{tr}.txt \\n{va}: ../{root}/{va}.txt \\n{te}: ../{root}/{te}.txt \\n # number of classes \\nnc: {len(classes)} \\n # class names \\nnames: {classes}\"\n",
    "\n",
    "file = open(\"yolov7/data/custom.yaml\", \"w\")\n",
    "file.write(custom_yaml)\n",
    "file.close()\n",
    "\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Read the file, update the 'nc' line, and rewrite the file\n",
    "with open(config_path, 'r') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "with open(config_path, 'w') as file:\n",
    "    for line in lines:\n",
    "        # Check if the line starts with 'nc:' and update it\n",
    "        if line.strip().startswith(\"nc:\"):\n",
    "            file.write(f\"nc: {num_classes}  # number of classes\\n\")\n",
    "        else:\n",
    "            file.write(line)\n",
    "\n",
    "print(f\"Num classes updated to {num_classes} in the config file.\")\n",
    "\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_path):\n",
    "    # Ensure the yolov7 directory exists\n",
    "    os.makedirs(\"yolov7\", exist_ok=True)\n",
    "    \n",
    "    # Download the file\n",
    "    os.system(f\"cd yolov7 && wget {file_url}\")\n",
    "    print(\"File downloaded successfully.\")\n",
    "else:\n",
    "    print(\"File already exists. No download necessary.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ac0f6f",
   "metadata": {},
   "source": [
    "This snippet configures the key runtime parameters for training the YOLOv7 model, adapting to the environment’s available hardware:\n",
    "\n",
    "1. **Device & Batch Size**  \n",
    "   - We check if there is a CUDA-compatible GPU (`torch.cuda.is_available()`). If so, we set:\n",
    "     - `DEVICE = 0` (indicating GPU index 0),\n",
    "     - `NUM_WORKERS = 1` for loading data in parallel, and\n",
    "     - `BATCH_SIZE = 16`, which is a reasonable size when a GPU is present for faster processing.  \n",
    "   - If no GPU is found, the code falls back to the CPU (`DEVICE = 'cpu'`), keeps `NUM_WORKERS = 1`, and reduces `BATCH_SIZE` to `2` to accommodate the lower computational throughput on CPU-based systems.\n",
    "\n",
    "2. **Image Size**  \n",
    "   - `IMG_SIZE = 640` defines the width and height to which the images will be resized. In YOLO-type models, 640×640 is a typical default for balanced performance and accuracy.\n",
    "\n",
    "3. **Setting Environment Variables**  \n",
    "   - We store these values (device, batch size, image size, etc.) in environment variables (`os.environ`) such as `YOLOV7_DEVICE`, `YOLOV7_BATCH_SIZE`, etc. This makes them easily accessible for downstream scripts and ensures consistent usage of the same hyperparameters when running training and inference commands.\n",
    "\n",
    "By configuring these parameters automatically based on CUDA availability, the training pipeline can seamlessly adjust to different hardware setups without manually changing code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d2b2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    DEVICE = 0\n",
    "    NUM_WORKERS = 1\n",
    "    BATCH_SIZE = 64\n",
    "else:\n",
    "    DEVICE = 'cpu'\n",
    "    NUM_WORKERS = 1\n",
    "    BATCH_SIZE = 2\n",
    "\n",
    "IMG_SIZE = 640\n",
    "\n",
    "os.environ['YOLOV7_DEVICE'] = str(DEVICE)\n",
    "os.environ['YOLOV7_NUM_WORKERS'] = str(NUM_WORKERS)\n",
    "os.environ['YOLOV7_BATCH_SIZE'] = str(BATCH_SIZE)\n",
    "os.environ['YOLOV7_IMG_SIZE'] = str(IMG_SIZE)\n",
    "os.environ['YOLOV7_FLAVOR'] = model_flavor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3495bbfa",
   "metadata": {},
   "source": [
    "***\n",
    "This command launches a YOLOv7 training run from the **`yolov7`** directory with several key arguments:\n",
    "\n",
    "1. **Change Directory**  \n",
    "   - `cd yolov7` ensures that we move into the `yolov7` folder, where the main training script and configurations reside.\n",
    "\n",
    "2. **W&B Offline**  \n",
    "   - `python3 -m wandb offline` disables Weights & Biases (W&B) online logging, forcing local/offline logging instead. This is helpful if you don’t want to upload logs or metrics to W&B servers.\n",
    "\n",
    "3. **Training Script (`train.py`)**  \n",
    "   - `--workers $YOLOV7_NUM_WORKERS`: Sets the number of data-loading workers using the environment variable `YOLOV7_NUM_WORKERS`.\n",
    "   - `--device $YOLOV7_DEVICE`: Defines the device for training (either GPU index or `'cpu'`), pulled from `YOLOV7_DEVICE`.\n",
    "   - `--batch-size $YOLOV7_BATCH_SIZE`: Uses the `YOLOV7_BATCH_SIZE` environment variable, which might differ depending on GPU vs. CPU mode.\n",
    "   - `--data data/custom.yaml`: Points to the custom dataset configuration file, which tells YOLOv7 where to find images/labels and how many classes to expect.\n",
    "   - `--img $YOLOV7_IMG_SIZE $YOLOV7_IMG_SIZE`: Specifies image width and height (both equal to `YOLOV7_IMG_SIZE`) for model training.\n",
    "   - `--cfg cfg/training/${YOLOV7_FLAVOR}.yaml`: Selects the YOLOv7 configuration file (e.g., `yolov7.yaml` or `yolov7-tiny.yaml`) based on `YOLOV7_FLAVOR`.\n",
    "   - `--weights ${YOLOV7_FLAVOR}.pt`: Loads pre-trained weights (e.g., `yolov7.pt` or `yolov7-tiny.pt`) before fine-tuning on the new dataset.\n",
    "   - `--name sima-${YOLOV7_FLAVOR}`: Assigns a custom name to this training run, useful for organizing results in logs.\n",
    "   - `--hyp data/hyp.scratch.custom.yaml`: Provides custom hyperparameters (learning rate, momentum, etc.) for training.\n",
    "   - `--epochs 10`: Sets the number of training epochs.\n",
    "\n",
    "Putting it all together, this command **trains** the YOLOv7 model on our dataset for 10 epochs, with the user-defined batch size, device selection, and custom dataset/hyperparameters, while storing all outputs in the `runs/` directory under a distinctive run name.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e5b52b-5769-4a75-b668-be16afb44c0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!cd yolov7 && python3 -m wandb offline && python3 train.py --workers $YOLOV7_NUM_WORKERS --device $YOLOV7_DEVICE --batch-size $YOLOV7_BATCH_SIZE --data data/custom.yaml --img $YOLOV7_IMG_SIZE $YOLOV7_IMG_SIZE --cfg cfg/training/${YOLOV7_FLAVOR}.yaml --weights ${YOLOV7_FLAVOR}.pt --name sima-${YOLOV7_FLAVOR} --hyp data/hyp.scratch.custom.yaml --epochs 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef69940d-f0c8-4edf-b285-5fd527c7851e",
   "metadata": {},
   "source": [
    "# ML Model ONNX Export\n",
    "\n",
    "This code Cell retrieves the path of the most recent training run for `YOLOv7`.\n",
    "\n",
    "1. `path` is defined as `'yolov7/runs/train/'`, where all training run results are stored.\n",
    "2. `all_runs` is a list comprehension that gathers paths to all directories within `path`, filtering for those that are directories.\n",
    "3. `latest_yolov7_run` finds the latest directory by selecting the one with the most recent modification time, using `max()` with `os.path.getmtime` as the key function.\n",
    "\n",
    "This process provides the path to the latest `YOLOv7` training output, which is useful for accessing model results, logs, or performance metrics from the most recent run.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d90238b-b0e8-43c2-b237-9ef38d406bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'yolov7/runs/train/'\n",
    "all_runs = [path + d for d in os.listdir(path) if os.path.isdir(path + d)]\n",
    "latest_yolov7_run = max(all_runs, key=os.path.getmtime)\n",
    "latest_yolov7_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c03394-bb32-4f6b-91ca-547854c1dd66",
   "metadata": {},
   "source": [
    "This code Cell sets up additional parameters and environment variables for evaluating `YOLOv7` model predictions.\n",
    "\n",
    "1. `TOP_K` is set to 100, specifying the maximum number of detections to retain per image. This limits the number of predictions to the top 100 with the highest confidence scores.\n",
    "2. `IOU_THR` (Intersection Over Union Threshold) is set to 0.65, defining the minimum overlap required between predicted and ground-truth bounding boxes for a positive match.\n",
    "3. `CONF_THR` (Confidence Threshold) is set to 0.35, filtering out predictions with confidence scores below this threshold.\n",
    "\n",
    "Each parameter is stored as an environment variable (`YOLOV7_TOP_K`, `YOLOV7_IOU_THR`, and `YOLOV7_CONF_THR`) to make them accessible in downstream evaluation scripts.\n",
    "\n",
    "Additionally, `YOLOV7_FILES` is assigned `latest_yolov7_run`, linking it to the path of the most recent training run. This setup enables streamlined access to the model’s outputs and allows for consistency across different evaluation stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b8d44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP_K = 100\n",
    "IOU_THR = 0.65\n",
    "CONF_THR = 0.30\n",
    "\n",
    "os.environ['YOLOV7_TOP_K'] = str(TOP_K)\n",
    "os.environ['YOLOV7_IOU_THR'] = str(IOU_THR)\n",
    "os.environ['YOLOV7_CONF_THR'] = str(CONF_THR)\n",
    "\n",
    "os.environ['YOLOV7_FILES'] = latest_yolov7_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4bc05de",
   "metadata": {},
   "source": [
    "***\n",
    "This code Cell exports the trained `YOLOv7` model for optimized deployment.\n",
    "\n",
    "1. The command `cd yolov7` navigates to the `yolov7` directory, where the `export.py` script is located.\n",
    "2. `python3 export.py` initiates the export process, taking several command-line arguments for customization:\n",
    "   - `--weights` specifies the path to the best model weights from the latest training run (`best.pt`).\n",
    "   - `--grid` enables the grid output, aligning model output with grid cells for improved precision.\n",
    "   - `--end2end` prepares the model for end-to-end deployment, packaging all necessary components.\n",
    "   - `--simplify` simplifies the model’s computation graph, enhancing inference efficiency.\n",
    "   - `--topk-all`, `--iou-thres`, and `--conf-thres` set the maximum detections per image, Intersection Over Union threshold, and confidence threshold, respectively, based on previously defined environment variables.\n",
    "   - `--img-size` defines the input image size for the exported model, set to `YOLOV7_IMG_SIZE`.\n",
    "   - `--max-wh` limits the width and height of bounding boxes to `YOLOV7_IMG_SIZE`, ensuring consistency in detection scaling.\n",
    "\n",
    "This setup exports an optimized, end-to-end `YOLOv7` model ready for deployment with enhanced performance, tailored thresholds, and grid alignment.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9938eb-724a-4832-af49-95c78dcd6941",
   "metadata": {},
   "outputs": [],
   "source": [
    "!cd yolov7 && python3 export.py --weights \"../${YOLOV7_FILES}/weights/best.pt\" --grid --end2end --simplify --topk-all $YOLOV7_TOP_K --iou-thres $YOLOV7_IOU_THR --conf-thres $YOLOV7_CONF_THR --img-size $YOLOV7_IMG_SIZE $YOLOV7_IMG_SIZE --max-wh $YOLOV7_IMG_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c48820d",
   "metadata": {},
   "source": [
    "***\n",
    "# Graph surgery\n",
    "\n",
    "In this cell, we perform a **“graph surgery”** on our YOLOv7 ONNX model. The initial goal is to remove certain layers in the post-processing head (such as `Reshape`, `Transpose`, `Split`, and `Concat`) that may be **unsupported** in downstream frameworks or accelerators. These layers often manipulate detection outputs into the final bounding-box format but aren’t strictly necessary if we can maintain a 4D layout through simpler operations.\n",
    "\n",
    "**First**, we locate the node prefix using `find_prefix_for_operation()` so we can systematically identify and remove the unwanted layers (`remove_nodes()`). By carefully removing the legacy YOLO layers (e.g., `Reshape` and `Transpose`), we make room for new 1×1 convolutions that achieve the same effect. This step streamlines the graph so that each detection scale (80×80, 40×40, and 20×20) remains a **4D tensor** without tricky reshapes.\n",
    "\n",
    "**Second**, we create and insert **point-wise convolution** nodes (`insert_pointwise_conv()`) to transform the original detection output channels (`xy`, `wh`, `conf`) in place of the removed layers. These 1×1 convolutions are easier to optimize on certain hardware, and they keep the network consistent. We also update constant tensors (`update_elmtwise_const()`) so the re-labeled graph accurately processes bounding-box predictions for the newly defined structure.\n",
    "\n",
    "**Finally**, we eliminate any references to **hardcoded dimensions** like 255 in the model. This can be crucial for certain hardware or for converting the model into different inference formats that don’t support fixed shape constraints. In the end, we run shape inference to confirm the model’s integrity (`onnx.shape_inference.infer_shapes(model)`) and save the refined ONNX file. With these modifications, the final graph becomes more portable and hardware-friendly, while still producing the same detection outputs in a more streamlined format.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a52610-75b3-4651-bcac-0f8d3df85319",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import onnx\n",
    "from onnx import numpy_helper\n",
    "\n",
    "# Goal\n",
    "#   Replace the (reshape + transpose + split) with point-wise convolution.\n",
    "#   Keep at 4D tensors for all layer outputs. \n",
    "#   Remove p3, p4, p5 model outputs.\n",
    "\n",
    "def find_prefix_for_operation(model, op_type):\n",
    "    \"\"\"\n",
    "    Find the prefix for nodes of a specific operation type.\n",
    "\n",
    "    Args:\n",
    "        model (onnx.ModelProto): Loaded ONNX model.\n",
    "        op_type (str): Operation type to search for (e.g., \"Reshape\").\n",
    "\n",
    "    Returns:\n",
    "        set: A set of unique prefixes for the specified operation type.\n",
    "    \"\"\"\n",
    "    prefixes = set()\n",
    "\n",
    "    # Iterate over nodes and check operation type\n",
    "    for node in model.graph.node:\n",
    "        if node.op_type == op_type and \"/\" in node.name:\n",
    "            # Extract prefix up to the second slash\n",
    "            parts = node.name.split(\"/\")\n",
    "            if len(parts) > 2:\n",
    "                prefix = \"/\".join(parts[:3])  # Example: '/model/model.xxx'\n",
    "                prefixes.add(prefix)\n",
    "\n",
    "    return prefixes.pop()\n",
    "\n",
    "# Remove reshape + transpose + split\n",
    "def remove_nodes(model):\n",
    "    remove_node_list = [\n",
    "        f\"{MODEL_PREFIX}/Reshape\",\n",
    "        f\"{MODEL_PREFIX}/Constant_2\",\n",
    "        f\"{MODEL_PREFIX}/Transpose\",\n",
    "        f\"{MODEL_PREFIX}/Split\",\n",
    "        f\"{MODEL_PREFIX}/Concat\",\n",
    "        f\"{MODEL_PREFIX}/Reshape_1\",\n",
    "        f\"{MODEL_PREFIX}/Constant_7\",\n",
    "        f\"{MODEL_PREFIX}/Reshape_2\",\n",
    "        f\"{MODEL_PREFIX}/Constant_8\",\n",
    "        f\"{MODEL_PREFIX}/Transpose_1\",\n",
    "        f\"{MODEL_PREFIX}/Split_1\",\n",
    "        f\"{MODEL_PREFIX}/Concat_1\",\n",
    "        f\"{MODEL_PREFIX}/Reshape_3\",\n",
    "        f\"{MODEL_PREFIX}/Reshape_4\",\n",
    "        f\"{MODEL_PREFIX}/Constant_14\",\n",
    "        f\"{MODEL_PREFIX}/Transpose_2\",\n",
    "        f\"{MODEL_PREFIX}/Split_2\",\n",
    "        f\"{MODEL_PREFIX}/Concat_2\",\n",
    "        f\"{MODEL_PREFIX}/Reshape_5\",\n",
    "        f\"{MODEL_PREFIX}/Constant\",\n",
    "        f\"{MODEL_PREFIX}/Constant_1\",\n",
    "    ]\n",
    "\n",
    "    remove_after_node_id = None\n",
    "    for node_id, node in enumerate(list(model.graph.node)):\n",
    "        if node.name in remove_node_list:\n",
    "            model.graph.node.remove(node)\n",
    "        elif node.name == f\"{MODEL_PREFIX}/Concat_3\":\n",
    "            remove_after_node_id = node_id\n",
    "            model.graph.node.remove(node)\n",
    "        elif remove_after_node_id is not None and node_id > remove_after_node_id:\n",
    "            model.graph.node.remove(node)\n",
    "\n",
    "\n",
    "# Create point-wise convolution nodes.\n",
    "def insert_pointwise_conv(model):\n",
    "    def _create_conv_node(base_node_name, size, input_name, output_name):\n",
    "        weight_name_prefix = base_node_name\n",
    "        node_name = f\"{base_node_name}_{size}\"\n",
    "        node = onnx.helper.make_node(\n",
    "            name=node_name, op_type=\"Conv\",\n",
    "            inputs=[input_name, f\"{weight_name_prefix}/weight:0\"],\n",
    "            outputs=[output_name], kernel_shape=(1, 1), dilations=(1, 1),\n",
    "            strides=(1, 1), pads=(0, 0, 0, 0))\n",
    "        return node\n",
    "\n",
    "    NUM_CLASSES = len(classes)\n",
    "    CHAN_PER_DET = 5 + NUM_CLASSES  # (x, y, w, h, obj + NUM_CLASSES)\n",
    "    base = 0\n",
    "\n",
    "    for idx, size in enumerate([80, 40, 20]):\n",
    "        # Node names depend on the detection head\n",
    "        node_name = {\n",
    "            80: f\"{MODEL_PREFIX}/Sigmoid\",\n",
    "            40: f\"{MODEL_PREFIX}/Sigmoid_1\",\n",
    "            20: f\"{MODEL_PREFIX}/Sigmoid_2\",\n",
    "        }[size]\n",
    "\n",
    "        conv_node_name = {\n",
    "            80: f\"{MODEL_PREFIX}/m.0/Conv\",\n",
    "            40: f\"{MODEL_PREFIX}/m.1/Conv\",\n",
    "            20: f\"{MODEL_PREFIX}/m.2/Conv\",\n",
    "        }[size]\n",
    "\n",
    "        for node_id, node in enumerate(list(model.graph.node)):\n",
    "            if node.name != node_name:\n",
    "                continue\n",
    "\n",
    "            node.input[0] = f\"{conv_node_name}_output_0\"\n",
    "\n",
    "            input_name = f\"{node_name}_output_0\"\n",
    "            output_name = f\"{MODEL_PREFIX}/Split_{idx}\" if idx > 0 else f\"{MODEL_PREFIX}/Split\"\n",
    "\n",
    "            # Add convolution nodes for xy, wh, and conf\n",
    "            model.graph.node.insert(\n",
    "                node_id + 1,\n",
    "                _create_conv_node(\"xy_conv\", size, input_name, f\"{output_name}_output_0\"))\n",
    "            model.graph.node.insert(\n",
    "                node_id + 2,\n",
    "                _create_conv_node(\"wh_conv\", size, input_name, f\"{output_name}_output_1\"))\n",
    "            model.graph.node.insert(\n",
    "                node_id + 3,\n",
    "                _create_conv_node(\"conf1_conv\", size, input_name, f\"{output_name}_output_2\"))\n",
    "            break\n",
    "    \n",
    "    base = 0\n",
    "    # Create and append weight tensors\n",
    "    for size_group in [2, 2, (1 + NUM_CLASSES)]:\n",
    "        data = np.zeros((3 * size_group, 3 * CHAN_PER_DET, 1, 1), dtype=np.float32)\n",
    "        for idx in range(3 * size_group):\n",
    "            src_idx = base + (idx // size_group) * CHAN_PER_DET + (idx % size_group)\n",
    "            dst_idx = idx\n",
    "            data[dst_idx, src_idx, 0, 0] = 1\n",
    "\n",
    "        name = {\n",
    "            0: \"xy_conv/weight:0\",\n",
    "            2: \"wh_conv/weight:0\",\n",
    "            4: \"conf1_conv/weight:0\",\n",
    "        }[base]\n",
    "\n",
    "        model.graph.initializer.append(onnx.helper.make_tensor(\n",
    "            name=name,\n",
    "            data_type=onnx.TensorProto.FLOAT,\n",
    "            dims=data.shape,\n",
    "            vals=data.flatten().tolist()))\n",
    "        base += size_group\n",
    "\n",
    "\n",
    "def update_elmtwise_const(model):\n",
    "    for node in model.graph.initializer:\n",
    "        if node.name in [\n",
    "                f\"{MODEL_PREFIX}/Constant_4_output_0\",\n",
    "                f\"{MODEL_PREFIX}/Constant_6_output_0\",\n",
    "                f\"{MODEL_PREFIX}/Constant_10_output_0\",\n",
    "                f\"{MODEL_PREFIX}/Constant_12_output_0\",\n",
    "                f\"{MODEL_PREFIX}/Constant_16_output_0\",\n",
    "                f\"{MODEL_PREFIX}/Constant_18_output_0\",\n",
    "            ]:\n",
    "            data = numpy_helper.to_array(node)\n",
    "            shape = data.shape\n",
    "            data = data.transpose(0, 1, 4, 2, 3).reshape(-1, shape[2], shape[3])\n",
    "\n",
    "            if node.name in [\n",
    "                    f\"{MODEL_PREFIX}/Constant_4_output_0\",\n",
    "                    f\"{MODEL_PREFIX}/Constant_10_output_0\",\n",
    "                    f\"{MODEL_PREFIX}/Constant_16_output_0\",\n",
    "                ]:\n",
    "                data = np.tile(data, (3, 1, 1))\n",
    "            node.CopyFrom(numpy_helper.from_array(data, node.name))\n",
    "\n",
    "\n",
    "def update_output_nodes(model):\n",
    "    NUM_CLASSES = len(classes)\n",
    "    CHAN_PER_DET = 1 + NUM_CLASSES\n",
    "\n",
    "    for node in list(model.graph.output):\n",
    "        model.graph.output.remove(node)\n",
    "\n",
    "    for size in [80, 40, 20]:\n",
    "        model.graph.output.append(onnx.helper.make_tensor_value_info(\n",
    "            f\"xy_{size}\", onnx.TensorProto.FLOAT,\n",
    "            (1, 3 * 2, size, size)))\n",
    "        model.graph.output.append(onnx.helper.make_tensor_value_info(\n",
    "            f\"wh_{size}\", onnx.TensorProto.FLOAT,\n",
    "            (1, 3 * 2, size, size)))\n",
    "        model.graph.output.append(onnx.helper.make_tensor_value_info(\n",
    "            f\"conf1_{size}\", onnx.TensorProto.FLOAT,\n",
    "            (1, 3 * CHAN_PER_DET, size, size)))\n",
    "\n",
    "    # Map the new outputs\n",
    "    name_map = {\n",
    "        f\"{MODEL_PREFIX}/Add\": \"xy_80\",\n",
    "        f\"{MODEL_PREFIX}/Mul_1\": \"wh_80\",\n",
    "        \"conf1_conv_80\": \"conf1_80\",\n",
    "\n",
    "        f\"{MODEL_PREFIX}/Add_1\": \"xy_40\",\n",
    "        f\"{MODEL_PREFIX}/Mul_3\": \"wh_40\",\n",
    "        \"conf1_conv_40\": \"conf1_40\",\n",
    "\n",
    "        f\"{MODEL_PREFIX}/Add_2\": \"xy_20\",\n",
    "        f\"{MODEL_PREFIX}/Mul_5\": \"wh_20\",\n",
    "        \"conf1_conv_20\": \"conf1_20\",\n",
    "    }\n",
    "    for node in model.graph.node:\n",
    "        if node.name in name_map:\n",
    "            node.output[0] = name_map[node.name]\n",
    "\n",
    "\n",
    "def remove_io_shape_constraints(model, old_dim=255):\n",
    "    \"\"\"\n",
    "    Removes (or sets to dynamic) any dimension == old_dim in graph input/output shapes.\n",
    "    \"\"\"\n",
    "    # --- Fix graph inputs ---\n",
    "    for inp in model.graph.input:\n",
    "        tensor_type = inp.type.tensor_type\n",
    "        if not tensor_type.HasField(\"shape\"):\n",
    "            continue\n",
    "        for d in tensor_type.shape.dim:\n",
    "            if d.HasField(\"dim_value\") and d.dim_value == old_dim:\n",
    "                # Option A: remove entire input if you no longer want to fix its shape\n",
    "                # model.graph.input.remove(inp)\n",
    "                # break  # or continue if you have multiple references\n",
    "\n",
    "                # Option B: set that dimension to dynamic\n",
    "                d.ClearField(\"dim_value\")\n",
    "\n",
    "    # --- Fix graph outputs ---\n",
    "    for out in model.graph.output:\n",
    "        tensor_type = out.type.tensor_type\n",
    "        if not tensor_type.HasField(\"shape\"):\n",
    "            continue\n",
    "        for d in tensor_type.shape.dim:\n",
    "            if d.HasField(\"dim_value\") and d.dim_value == old_dim:\n",
    "                # remove or set dynamic\n",
    "                d.ClearField(\"dim_value\")\n",
    "\n",
    "def remove_node_attribute_shapes(model, old_dim=255):\n",
    "    for node in model.graph.node:\n",
    "        for attr in node.attribute:\n",
    "            # Many shape-related attributes are stored as 'ints' or 'tensors'.\n",
    "            if attr.type == onnx.AttributeProto.INTS:\n",
    "                # E.g. Reshape's \"shape\" attribute might be here\n",
    "                shape_list = list(attr.ints)\n",
    "                # If 255 is in shape_list, we remove or fix it\n",
    "                if old_dim in shape_list:\n",
    "                    # Option A: remove 255 entirely\n",
    "                    # shape_list = [x for x in shape_list if x != old_dim]\n",
    "                    \n",
    "                    # Option B: replace 255 with dynamic (-1)\n",
    "                    shape_list = [(-1 if x == old_dim else x) for x in shape_list]\n",
    "                    attr.ints[:] = shape_list\n",
    "            elif attr.type == onnx.AttributeProto.TENSOR:\n",
    "                # E.g., a Constant node that might define a shape tensor\n",
    "                arr = onnx.numpy_helper.to_array(attr.t)\n",
    "                if old_dim in arr:\n",
    "                    # fix the array\n",
    "                    arr = np.where(arr == old_dim, -1, arr)\n",
    "                    attr.t.CopyFrom(onnx.numpy_helper.from_array(arr))                \n",
    "\n",
    "def remove_value_info_shapes(model, old_dim=255):\n",
    "    # We can either remove them entirely or fix that dimension.\n",
    "    to_remove = []\n",
    "    for vi in model.graph.value_info:\n",
    "        tensor_type = vi.type.tensor_type\n",
    "        if not tensor_type.HasField(\"shape\"):\n",
    "            continue\n",
    "\n",
    "        # Check if shape includes old_dim\n",
    "        found_255 = False\n",
    "        for d in tensor_type.shape.dim:\n",
    "            if d.HasField(\"dim_value\") and d.dim_value == old_dim:\n",
    "                found_255 = True\n",
    "                break\n",
    "\n",
    "        if found_255:\n",
    "            # Option A: remove the entire value_info entry\n",
    "            to_remove.append(vi)\n",
    "            # Option B: fix dimension to dynamic\n",
    "            # for d in tensor_type.shape.dim:\n",
    "            #     if d.HasField(\"dim_value\") and d.dim_value == old_dim:\n",
    "            #         d.ClearField(\"dim_value\")\n",
    "\n",
    "    # Remove them from the graph\n",
    "    for vi in to_remove:\n",
    "        model.graph.value_info.remove(vi)\n",
    "\n",
    "def remove_initializer_shapes(model, old_dim=255):\n",
    "    to_remove = []\n",
    "    for init in model.graph.initializer:\n",
    "        dims = list(init.dims)\n",
    "        if old_dim in dims:\n",
    "            # Option A: remove the initializer\n",
    "            to_remove.append(init)\n",
    "            # Option B: fix dims from 255 -> -1 or the new dimension\n",
    "            # dims = [new_dim if x == old_dim else x for x in dims]\n",
    "            # Re-assign\n",
    "            # init.ClearField(\"dims\")\n",
    "            # init.dims.extend(dims)\n",
    "\n",
    "    for init in to_remove:\n",
    "        model.graph.initializer.remove(init)\n",
    "\n",
    "def remove_all_255_shapes(model, old_dim=255):\n",
    "    # 1) Fix or remove I/O shapes\n",
    "    remove_io_shape_constraints(model, old_dim)\n",
    "\n",
    "    # 2) Fix or remove ValueInfo\n",
    "    remove_value_info_shapes(model, old_dim)\n",
    "\n",
    "    # 3) Fix or remove any attribute shapes\n",
    "    remove_node_attribute_shapes(model, old_dim)\n",
    "\n",
    "    # 4) Fix or remove initializers referencing 255\n",
    "    remove_initializer_shapes(model, old_dim)\n",
    "\n",
    "model_name = latest_yolov7_run + \"/weights/best\"\n",
    "model = onnx.load(f\"{model_name}.onnx\")\n",
    "\n",
    "MODEL_PREFIX = find_prefix_for_operation(model, \"Reshape\")\n",
    "\n",
    "ONNX_MODEL_NAME = latest_yolov7_run + \"/yolov7.onnx\"\n",
    "\n",
    "remove_nodes(model)\n",
    "insert_pointwise_conv(model)\n",
    "update_elmtwise_const(model)\n",
    "update_output_nodes(model)\n",
    "\n",
    "# Remove the existing shapes.\n",
    "for node in list(model.graph.value_info):\n",
    "    model.graph.value_info.remove(node)\n",
    "\n",
    "remove_all_255_shapes(model, old_dim=255)\n",
    "\n",
    "model = onnx.shape_inference.infer_shapes(model)\n",
    "onnx.checker.check_model(model)\n",
    "onnx.save(model, ONNX_MODEL_NAME)\n",
    "\n",
    "print(model_name)\n",
    "print(\"Graph surgery completed successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4841dfff",
   "metadata": {},
   "source": [
    "***\n",
    "# Quantized Model Inference\n",
    "\n",
    "This section imports the necessary libraries and modules, including `argparse`, `os`, `cv2`, `numpy`, and `torch`. It also imports specific functions and classes from various submodules, which will be used later in the script for handling image processing, model loading, and evaluation.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6953cec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from afe.ir.defines import InputName\n",
    "from afe.ir.tensor_type import ScalarType\n",
    "from afe.load.importers.general_importer import onnx_source\n",
    "from sima_utils.data.data_generator import DataGenerator\n",
    "import onnxruntime as ort\n",
    "\n",
    "from afe.apis.defines import default_quantization, HistogramMSEMethod, quantization_scheme, dataclasses\n",
    "from afe.apis.loaded_net import load_model\n",
    "from afe.core.utils import convert_data_generator_to_iterable\n",
    "from afe.core.evaluate_networks import GraphEvaluatorLogger\n",
    "\n",
    "import torchvision\n",
    "from IPython.display import display\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34c88ed",
   "metadata": {},
   "source": [
    "***\n",
    "Here, we define constants for the dataset path and the number of samples to be processed. The `DATASET_PATH` variable points to the location of validation labels, while `NUM_OF_SAMPLES` specifies how many samples will be utilized.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90338f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH =  'new_data/labels/val/'\n",
    "NUM_OF_SAMPLES = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f28ef51c",
   "metadata": {},
   "source": [
    "***\n",
    "This function `clip_coords` adjusts the bounding box coordinates so they don't go outside the image boundaries. It takes `boxes` as input, which are in the format `[x1, y1, x2, y2]`, and ensures that all coordinates are within the valid range defined by `img_shape`.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79d73e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_coords(boxes, img_shape):\n",
    "    # Clip bounding xyxy bounding boxes to image shape (height, width)\n",
    "    boxes[:, 0].clamp_(0, img_shape[1])  # x1\n",
    "    boxes[:, 1].clamp_(0, img_shape[0])  # y1\n",
    "    boxes[:, 2].clamp_(0, img_shape[1])  # x2\n",
    "    boxes[:, 3].clamp_(0, img_shape[0])  # y2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662d144",
   "metadata": {},
   "source": [
    "***\n",
    "The `scale_coords` function rescales bounding box coordinates from one image size to another. It calculates the scaling factor and padding based on the dimensions of the source (`img1_shape`) and destination (`img0_shape`) images. The `clip_coords` function is called at the end to ensure the scaled coordinates are valid.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30b20b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_coords(img1_shape, coords, img0_shape, ratio_pad=None):\n",
    "    # Rescale coords (xyxy) from img1_shape to img0_shape\n",
    "    if ratio_pad is None:  # calculate from img0_shape\n",
    "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n",
    "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n",
    "    else:\n",
    "        gain = ratio_pad[0][0]\n",
    "        pad = ratio_pad[1]\n",
    "\n",
    "    coords[:, [0, 2]] -= pad[0]  # x padding\n",
    "    coords[:, [1, 3]] -= pad[1]  # y padding\n",
    "    coords[:, :4] /= gain\n",
    "    clip_coords(coords, img0_shape)\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d17ac",
   "metadata": {},
   "source": [
    "***\n",
    "The `xywh2xyxy` function converts bounding box coordinates from the format `[center_x, center_y, width, height]` to `[x1, y1, x2, y2]`, where `x1, y1` is the top-left corner and `x2, y2` is the bottom-right corner. This format is often required for further processing.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c04dc30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xywh2xyxy(x):\n",
    "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
    "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719b0517",
   "metadata": {},
   "source": [
    "***\n",
    "The `box_iou` function calculates the Intersection over Union (IoU) between two sets of bounding boxes. It first computes the area of each box and then determines the intersection area. The IoU is returned as a measure of how much the boxes overlap.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703c63f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def box_iou(box1, box2):\n",
    "    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
    "    \"\"\"\n",
    "    Return intersection-over-union (Jaccard index) of boxes.\n",
    "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
    "    Arguments:\n",
    "        box1 (Tensor[N, 4])\n",
    "        box2 (Tensor[M, 4])\n",
    "    Returns:\n",
    "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
    "            IoU values for every element in boxes1 and boxes2\n",
    "    \"\"\"\n",
    "\n",
    "    def box_area(box):\n",
    "        # box = 4xn\n",
    "        return (box[2] - box[0]) * (box[3] - box[1])\n",
    "\n",
    "    area1 = box_area(box1.T)\n",
    "    area2 = box_area(box2.T)\n",
    "\n",
    "    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
    "    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n",
    "    return inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d5eef3",
   "metadata": {},
   "source": [
    "***\n",
    "The `non_max_suppression` function applies the NMS algorithm to filter out overlapping bounding boxes based on confidence scores and IoU thresholds. It processes each image's predictions, applying constraints and aggregating the results into a final output.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab84d031-eee9-4f0a-9dd3-426c04ce8970",
   "metadata": {},
   "outputs": [],
   "source": [
    "def non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False,\n",
    "                        labels=()):\n",
    "    \"\"\"Runs Non-Maximum Suppression (NMS) on inference results\n",
    "\n",
    "    Returns:\n",
    "         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
    "    \"\"\"\n",
    "\n",
    "    nc = prediction.shape[2] - 5  # number of classes\n",
    "    xc = prediction[..., 4] > conf_thres  # candidates\n",
    "\n",
    "    # Settings\n",
    "    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
    "    max_det = 300  # maximum number of detections per image\n",
    "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
    "    time_limit = 10.0  # seconds to quit after\n",
    "    redundant = True  # require redundant detections\n",
    "    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
    "    merge = False  # use merge-NMS\n",
    "\n",
    "    output = [torch.zeros((0, 6), device='cpu')] * prediction.shape[0]\n",
    "    for xi, x in enumerate(prediction):  # image index, image inference\n",
    "        # Apply constraints\n",
    "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
    "        x = x[xc[xi]]  # confidence\n",
    "\n",
    "        # Cat apriori labels if autolabelling\n",
    "        if labels and len(labels[xi]):\n",
    "            l = labels[xi]\n",
    "            v = torch.zeros((len(l), nc + 5), device=x.device)\n",
    "            v[:, :4] = l[:, 1:5]  # box\n",
    "            v[:, 4] = 1.0  # conf\n",
    "            v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
    "            x = torch.cat((x, v), 0)\n",
    "\n",
    "        # If none remain process next image\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        # Compute conf\n",
    "        if nc == 1:\n",
    "            x[:, 5:] = x[:, 4:5]  # for models with one class, cls_loss is 0 and cls_conf is always 0.5,\n",
    "            # so there is no need to multiplicate.\n",
    "        else:\n",
    "            x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
    "\n",
    "        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
    "        box = xywh2xyxy(x[:, :4])\n",
    "\n",
    "        # Detections matrix nx6 (xyxy, conf, cls)\n",
    "        if multi_label:\n",
    "            i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
    "            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
    "        else:  # best class only\n",
    "            conf, j = x[:, 5:].max(axis=1, keepdim=True)\n",
    "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
    "\n",
    "        # Filter by class\n",
    "        if classes is not None:\n",
    "            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
    "\n",
    "        # Check shape\n",
    "        n = x.shape[0]  # number of boxes\n",
    "        if not n:  # no boxes\n",
    "            continue\n",
    "        elif n > max_nms:  # excess boxes\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
    "\n",
    "        # Batched NMS\n",
    "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
    "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
    "        if i.shape[0] > max_det:  # limit detections\n",
    "            i = i[:max_det]\n",
    "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
    "            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
    "            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
    "            weights = iou * scores[None]  # box weights\n",
    "            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
    "            if redundant:\n",
    "                i = i[iou.sum(1) > 1]  # require redundancy\n",
    "\n",
    "        output[xi] = x[i]\n",
    "\n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ddc2e4",
   "metadata": {},
   "source": [
    "***\n",
    "This code Cell defines two functions, `preprocess` and `_load_model`, which prepare an image for model inference and load the `YOLOv7` ONNX model for evaluation.\n",
    "\n",
    "1. **`preprocess` function:** This function processes an input image to match the model's expected input shape.\n",
    "   - `img_h` and `img_w` capture the original image dimensions, while `new_h` and `new_w` are adjusted according to the specified `input_shape`.\n",
    "   - If `letter_box` is `True`, it applies \"letterbox\" resizing, preserving the original aspect ratio by adding padding. The resized image is placed on a grey (127) background to match `input_shape`, centering it with calculated offsets (`offset_h`, `offset_w`).\n",
    "   - If `letter_box` is `False`, the image is resized directly to `input_shape`.\n",
    "   - The image is then converted from BGR to RGB, transposed to channel-first format, normalized (dividing by 255.0), and expanded to a 4D tensor for model input.\n",
    "\n",
    "2. **`_load_model` function:** This function loads the ONNX model for inference.\n",
    "   - `model_path` is set to the path of the `YOLOv7` ONNX model (`ONNX_MODEL_NAME`).\n",
    "   - `shapes_dict` specifies the input shape of the model as `(1, 3, IMG_SIZE, IMG_SIZE)`, while `dtype_dict` sets the data type for the input.\n",
    "   - `onnx_source` retrieves model information, and `load_model` loads the ONNX model for use in inference.\n",
    "\n",
    "Finally, `loaded_net` calls `_load_model` to initialize the model, preparing it for inference on preprocessed images.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6553988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(img, input_shape, letter_box=True):\n",
    "    if letter_box:\n",
    "        img_h, img_w, _ = img.shape\n",
    "        new_h, new_w = input_shape[0], input_shape[1]\n",
    "        offset_h, offset_w = 0, 0\n",
    "        if (new_w / img_w) <= (new_h / img_h):\n",
    "            new_h = int(img_h * new_w / img_w)\n",
    "            offset_h = (input_shape[0] - new_h) // 2\n",
    "        else:\n",
    "            new_w = int(img_w * new_h / img_h)\n",
    "            offset_w = (input_shape[1] - new_w) // 2\n",
    "        resized = cv2.resize(img, (new_w, new_h))\n",
    "        img = np.full((input_shape[0], input_shape[1], 3), 127, dtype=np.uint8)\n",
    "        img[offset_h:(offset_h + new_h), offset_w:(offset_w + new_w), :] = resized\n",
    "    else:\n",
    "        img = cv2.resize(img, (input_shape[1], input_shape[0]))\n",
    "\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = img.transpose((2, 0, 1)).astype(np.float32)\n",
    "    img /= 255.0\n",
    "    img = np.expand_dims(img, 0)\n",
    "    return img\n",
    "\n",
    "\n",
    "def _load_model():\n",
    "    model_path = ONNX_MODEL_NAME\n",
    "    shapes_dict = {\"images\": (1, 3, IMG_SIZE, IMG_SIZE)}\n",
    "    dtype_dict = {\"images\": ScalarType.float32}\n",
    "\n",
    "    importer_params = onnx_source(model_path=model_path, shape_dict=shapes_dict, dtype_dict=dtype_dict)\n",
    "\n",
    "    loaded_net = load_model(importer_params)\n",
    "    return loaded_net\n",
    "\n",
    "from afe.apis.error_handling_variables import enable_verbose_error_messages\n",
    "\n",
    "enable_verbose_error_messages()\n",
    "\n",
    "loaded_net = _load_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b625c424",
   "metadata": {},
   "source": [
    "***\n",
    "# ML Model Quantization\n",
    "\n",
    "Now we create calibration data and perform model quantization to improve performance.\n",
    "\n",
    "- **`_make_calibration_data()`**: This function gathers images for calibration from the COCO validation dataset located in `/coco/val2017`. It processes the first 10 images in the dataset by reading them, resizing them to 640x640 pixels (with the help of the previously defined `preprocess` function), and storing them in a list. After processing, we concatenate all the images into a single array, preparing them for calibration. Finally, we convert this data into a format suitable for the model using `convert_data_generator_to_iterable`.\n",
    "\n",
    "- **Quantization Process**: After generating the calibration data, we set up the quantization configuration using the default settings with calibration based on the Histogram Mean Squared Error method. We then call `_make_calibration_data()` to retrieve our prepared calibration images. With this data in hand, we can quantize our loaded model (`loaded_net`) by passing in the calibration data and the quantization configuration, resulting in a more efficient model ready for deployment.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_calibration_data():\n",
    "    \"\"\"\n",
    "    Make calibration data using 35 samples from '/coco/val2017' dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    images = []\n",
    "    for filename in os.listdir(DATASET_PATH)[:10]:\n",
    "        image_path = os.path.join(DATASET_PATH, filename).replace(\"labels\", \"images\").replace(\".txt\", \".jpg\")\n",
    "        image = cv2.imread(image_path)\n",
    "        preprocessed_image1 = preprocess(img=image, input_shape=(IMG_SIZE, IMG_SIZE)).transpose(0, 2, 3, 1)\n",
    "        images.append(preprocessed_image1)\n",
    "\n",
    "    cal_images = np.concatenate(images, axis=0)\n",
    "\n",
    "    inputs = {InputName('images'): cal_images}\n",
    "\n",
    "    calibration_data = convert_data_generator_to_iterable(DataGenerator(inputs))\n",
    "\n",
    "    return calibration_data\n",
    "\n",
    "# Quantize model\n",
    "quant_configs = default_quantization.with_calibration(HistogramMSEMethod(num_bins=1024))\n",
    "calibration_data = _make_calibration_data()\n",
    "quantized_net = loaded_net.quantize(calibration_data=calibration_data, quantization_config=quant_configs, model_name=latest_yolov7_run.split(\"/\")[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17b1ca3",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "This section executes the model on a set of images, processes the predictions, and saves the quantized model.\n",
    "\n",
    "- **Initialization**: We start by setting the `QUANTIZED` and `ONNX` flags to determine which model execution path to follow. We also initialize a logger (`GraphEvaluatorLogger`) to track the progress of the evaluation.\n",
    "\n",
    "- **Processing Each Sample**: We iterate through a specified number of image samples from the dataset:\n",
    "  - The logger updates a progress bar to provide feedback on the current processing state.\n",
    "  - Each image is read and preprocessed to the required input shape of 640x640 pixels.\n",
    "\n",
    "- **Model Execution**: Based on the flags:\n",
    "  - If `QUANTIZED` is `True`, we execute the quantized model (`quantized_net`) with the preprocessed input.\n",
    "  - If `ONNX` is `True`, we prepare the input for the ONNX model and run the inference session using `onnxruntime`.\n",
    "  - If neither flag is set, we execute the standard loaded network (`loaded_net`).\n",
    "\n",
    "- **Output Handling**: The optimized model produces multiple outputs that need to be rearranged. This part of the code reshapes and concatenates the outputs to create a single prediction tensor:\n",
    "  - The predictions are then processed through a non-max suppression function to filter out overlapping bounding boxes based on a confidence threshold.\n",
    "\n",
    "- **Scaling Predictions**: The detected bounding box coordinates are scaled back to match the original image dimensions for accurate visualization.\n",
    "\n",
    "- **Final Output Preparation**: We extract bounding boxes, scores, and class labels from the predictions. The bounding box format is adjusted from `(xmin, ymin, xmax, ymax)` to `(ymin, xmin, ymax, xmax)` for compatibility with later processing.\n",
    "\n",
    "- **Model Saving and Compilation**: Finally, the quantized model is saved and compiled into the specified directory, ensuring it is ready for deployment.\n",
    "\n",
    "This structured approach ensures that images are efficiently processed and predictions are accurately made and stored.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48610fa0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "QUANTIZED = True\n",
    "ONNX = False\n",
    "\n",
    "ort.set_default_logger_severity(3)\n",
    "\n",
    "video_state = {}\n",
    "\n",
    "def load_and_preprocess_image(filename):\n",
    "    \"\"\"\n",
    "    Load and preprocess an image or the next frame from a video.\n",
    "\n",
    "    Args:\n",
    "        filename (str): Path to the image or video file.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (preprocessed_image, original_image/frame)\n",
    "    \n",
    "    Raises:\n",
    "        StopIteration: When no more frames are available in a video.\n",
    "    \"\"\"\n",
    "    if filename.endswith(\".mp4\"):\n",
    "        # Check if the video is already opened\n",
    "        if filename not in video_state:\n",
    "            cap = cv2.VideoCapture(filename)\n",
    "            if not cap.isOpened():\n",
    "                raise FileNotFoundError(f\"Cannot open video file: {filename}\")\n",
    "            video_state[filename] = cap\n",
    "        else:\n",
    "            cap = video_state[filename]\n",
    "\n",
    "        # Read the next frame\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:  # End of video\n",
    "            cap.release()\n",
    "            del video_state[filename]  # Clean up state\n",
    "            raise StopIteration(f\"No more frames in video: {filename}\")\n",
    "\n",
    "        # Preprocess the frame\n",
    "        preprocessed_frame = preprocess(img=frame, input_shape=(IMG_SIZE, IMG_SIZE)).transpose(0, 2, 3, 1)\n",
    "        return preprocessed_frame, frame\n",
    "\n",
    "    else:\n",
    "        # Handle image files\n",
    "        image_path = os.path.join(DATASET_PATH, filename).replace(\"labels\", \"images\").replace(\".txt\", \".jpg\")\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Cannot load image file: {image_path}\")\n",
    "        preprocessed_image = preprocess(img=image, input_shape=(IMG_SIZE, IMG_SIZE)).transpose(0, 2, 3, 1)\n",
    "        return preprocessed_image, image\n",
    "\n",
    "\n",
    "def execute_model(preprocessed_image):\n",
    "    \"\"\"Execute the model based on the selected mode (QUANTIZED or ONNX).\"\"\"\n",
    "    inputs = {InputName('images'): preprocessed_image}\n",
    "    \n",
    "    if QUANTIZED:\n",
    "        return quantized_net.execute(inputs)\n",
    "    elif ONNX:\n",
    "        ort_session = ort.InferenceSession(ONNX_MODEL_NAME)\n",
    "        \n",
    "        preprocessed_image = preprocessed_image.transpose(0, 3, 1, 2)\n",
    "        outputs = ort_session.run(None, {'images': preprocessed_image})\n",
    "\n",
    "        final_output = []\n",
    "        for output in outputs:\n",
    "            final_output.append(output.transpose(0, 2, 3, 1))\n",
    "        return final_output\n",
    "    else:\n",
    "        return loaded_net.execute(inputs)\n",
    "\n",
    "def process_and_draw_outputs(out, original_image):\n",
    "    \"\"\"Process the model outputs and draw predictions on the original image.\"\"\"\n",
    "    output = []\n",
    "    for i in range(3):\n",
    "        data = list()\n",
    "        for x in out[0 + i*3 : 3 + i*3]:\n",
    "            shape = x.shape\n",
    "            x = x.reshape(*shape[0:3], 3, shape[3] // 3).transpose(0, 3, 1, 2, 4)\n",
    "            x = x.reshape(shape[0], -1, shape[3] // 3)\n",
    "            data.append(x)\n",
    "        data = np.concatenate(data, axis=2)\n",
    "        output.append(data)\n",
    "\n",
    "    pred = np.concatenate(output, axis=1)\n",
    "\n",
    "    # Get predictions    \n",
    "    detections = get_detections(pred, original_image.shape)\n",
    "\n",
    "    return draw_detections(original_image, detections)\n",
    "\n",
    "def get_detections(pred, image_shape):\n",
    "    \"\"\"Get bounding box predictions and scale them to the original image size.\"\"\"\n",
    "    net_out = None\n",
    "    pred = non_max_suppression(prediction=torch.from_numpy(pred), conf_thres=0.18, iou_thres=0.0)\n",
    "\n",
    "    # Scale coordinates to match original picture\n",
    "    detections = []\n",
    "    for detections in pred:\n",
    "        detections[:, :4] = scale_coords((IMG_SIZE, IMG_SIZE), detections[:, :4], image_shape).round()\n",
    "\n",
    "    if detections is not None:\n",
    "        bbox, scores, classes_number = detections[:, :4], detections[:, 4:5], detections[:, 5:6]\n",
    "        bboxes = np.array(bbox)\n",
    "        net_out = [bboxes, scores.detach().cpu().numpy()[:, 0], [classes[int(class_n)] for class_n in classes_number]]\n",
    "\n",
    "    return net_out\n",
    "\n",
    "def draw_detections(image, detections):\n",
    "    \"\"\"Draw bounding boxes and labels on the image.\"\"\"\n",
    "    boxes = detections[0]\n",
    "    for i, box in enumerate(boxes):\n",
    "        x_min, y_min, x_max, y_max = map(int, box[:4])\n",
    "        score = detections[1][i]\n",
    "        label = detections[2][i]\n",
    "        color = (0, 255, 0)  # Green color for the boxes\n",
    "        cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "        cv2.putText(image, f'Class: {label} | Score: {score:.2f}', \n",
    "                    (x_min, y_min - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def run_inference(num_samples=NUM_OF_SAMPLES):\n",
    "    \"\"\"Run inference on a set number of samples from the dataset.\"\"\"\n",
    "    logger = GraphEvaluatorLogger(True, None)\n",
    "\n",
    "    for i, filename in enumerate(os.listdir(DATASET_PATH)[:5]):\n",
    "        logger.print_progressbar(i + 1, num_samples, \"\")\n",
    "        \n",
    "        # Load and preprocess image\n",
    "        preprocessed_image, original_image = load_and_preprocess_image(filename)\n",
    "\n",
    "        # Execute the model and process outputsfilename\n",
    "        out = execute_model(preprocessed_image)\n",
    "        image_with_detections = process_and_draw_outputs(out, original_image)\n",
    "\n",
    "        # Display the image with predictions in Jupyter Notebook\n",
    "        display(Image.fromarray(image_with_detections[:, :, ::-1]))\n",
    "\n",
    "# After defining the functions, call run_inference to execute predictions\n",
    "run_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523aee51-cf7f-4980-b590-c0969cdf5ff3",
   "metadata": {},
   "source": [
    "***\n",
    "# ML Model Compilation\n",
    "\n",
    "This code snippet handles saving and compiling the quantized YOLOv7 model, placing the compiled model into a specified directory.\n",
    "\n",
    "1. **Setting the Directory Path**:\n",
    "   - `saved_mpk_directory = latest_yolov7_run + \"/compiled_yolov7\"`:\n",
    "     - This line constructs the path where the compiled YOLOv7 model will be saved. It uses `latest_yolov7_run` to get the path to the most recent YOLOv7 training run and appends `/compiled_yolov7` to create a subdirectory for the compiled model.\n",
    "\n",
    "2. **Saving the Quantized Model**:\n",
    "   - `quantized_net.save(\"yolov7\", output_directory=saved_mpk_directory)`:\n",
    "     - This line saves the quantized YOLOv7 model (represented by `quantized_net`) to the specified directory (`saved_mpk_directory`). The model is saved under the name `\"yolov7\"`.\n",
    "\n",
    "3. **Compiling the Quantized Model**:\n",
    "   - `quantized_net.compile(output_path=saved_mpk_directory, compress=False)`:\n",
    "     - This line compiles the saved quantized model and stores the compiled files in the `saved_mpk_directory`. The `compress=False` argument ensures that the model is not compressed during the compilation process.\n",
    "\n",
    "This sequence of steps ensures that the quantized YOLOv7 model is saved and compiled into a format ready for deployment or further use.\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09b75ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "saved_mpk_directory = latest_yolov7_run + \"/compiled_yolov7\"\n",
    "quantized_net.save(\"yolov7\", output_directory=saved_mpk_directory)\n",
    "quantized_net.compile(output_path=saved_mpk_directory, compress=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a11a4e-f5ae-4271-9790-be03785747e0",
   "metadata": {},
   "source": [
    "***\n",
    "# Preparing the compiled model for Edgematic\n",
    "\n",
    "In this final step, we **upload** the compiled YOLOv7 model artifact to an Amazon S3 bucket and **generate a pre-signed URL** for easy access:\n",
    "\n",
    "1. **Extracting the Model Name and Path**  \n",
    "   - We parse `latest_yolov7_run` to get the run name (e.g., `exp7`) and construct the path to the compiled `.tar.gz` file. This archive contains the optimized, quantized, or compiled YOLOv7 model. \n",
    "\n",
    "2. **Uploading to S3 with boto3**  \n",
    "   - We import the `boto3` library, which provides a Python interface to AWS services. We create an S3 resource (`boto3.resource(\"s3\")`) and call `upload_file(...)` to transfer the `.tar.gz` file from our local machine to the specified S3 bucket (`s3_bucket`), storing it under a structured key (`models/<name>.tar.gz`).\n",
    "\n",
    "3. **Ensuring Easy Retrieval**  \n",
    "   - Once the file is in S3, we generate a **pre-signed URL** that will remain valid for 30 minutes. By calling `generate_presigned_url` on an S3 **client**, we get a unique link that can be shared to grant temporary download permission—no need to make the bucket publicly accessible.  \n",
    "   - This approach allows you or other collaborators to retrieve the YOLOv7 artifact without manually logging into AWS or altering S3 permissions.\n",
    "\n",
    "By **storing the compiled model in S3** and distributing a **time-limited pre-signed URL**, you ensure a secure, controlled mechanism for accessing the compiled YOLOv7 model—ideal for subsequent steps such as inference on AWS services or local testing in Edgematic.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13d015c9-065b-4409-a717-a2718b308419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "# --------------------------------\n",
    "# 1) Upload the compiled tar.gz file\n",
    "# --------------------------------\n",
    "\n",
    "name = latest_yolov7_run.split(\"/\")[-1]\n",
    "file_name = f\"{latest_yolov7_run}/compiled_yolov7/{name}_mpk.tar.gz\"\n",
    "object_key = f\"models/{name}.tar.gz\"\n",
    "\n",
    "s3 = boto3.resource('s3')\n",
    "s3.meta.client.upload_file(file_name, s3_bucket, object_key)\n",
    "\n",
    "print(f\"Uploaded {file_name} to s3://{s3_bucket}/{object_key}\")\n",
    "\n",
    "# --------------------------------\n",
    "# 2) Generate a 30-minute pre-signed URL\n",
    "# --------------------------------\n",
    "\n",
    "s3_client = boto3.client(\"s3\")\n",
    "\n",
    "presigned_url = s3_client.generate_presigned_url(\n",
    "    ClientMethod=\"get_object\",\n",
    "    Params={\n",
    "        \"Bucket\": s3_bucket,\n",
    "        \"Key\": object_key\n",
    "    },\n",
    "    ExpiresIn=1800  # 30 minutes\n",
    ")\n",
    "\n",
    "print(\"Pre-Signed URL (valid for 30 min):\")\n",
    "print(presigned_url)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
